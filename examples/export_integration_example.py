"""
BACOWR Export Integration Example

Shows how Google Sheets/Docs export integrates with the job workflow.
This demonstrates the COMPLETE integration - not a separate feature!
"""

import asyncio
import os
from dotenv import load_dotenv

# Import BACOWR job management (existing)
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from api.app.database import SessionLocal
from api.app.models.database import Job
from api.app.services.job_orchestrator import JobOrchestrator

# Import Google export (new, but integrated!)
from src.export import GoogleAuthManager, GoogleSheetsExporter, GoogleDocsExporter

load_dotenv()


async def example_1_single_job_with_export():
    """
    Example 1: Create job ‚Üí Run BACOWR ‚Üí Export to Google

    This shows the INTEGRATED workflow:
    1. Create job using existing JobOrchestrator
    2. Job runs through normal BACOWR pipeline
    3. After completion, auto-export to Google
    """
    print("=" * 70)
    print("EXAMPLE 1: Single Job with Auto-Export")
    print("=" * 70)

    # Step 1: Create job (EXISTING BACOWR functionality)
    orchestrator = JobOrchestrator()

    job_result = await orchestrator.create_job(
        publisher_domain="aftonbladet.se",
        target_url="https://example.com/product",
        anchor_text="best project management tools",
        llm_provider="anthropic"
    )

    job_id = job_result['job_id']
    print(f"‚úÖ Job created: {job_id}")

    # Step 2: Wait for completion (EXISTING workflow)
    # In real API, this happens via background task
    # Here we simulate it
    await orchestrator.wait_for_completion(job_id)
    print(f"‚úÖ Job completed")

    # Step 3: Export to Google (NEW, but uses existing Job data!)
    db = SessionLocal()
    try:
        job = db.query(Job).filter(Job.id == job_id).first()

        if job and job.status == "delivered":
            # Initialize exporters
            auth = GoogleAuthManager()
            sheets_exporter = GoogleSheetsExporter(auth)
            docs_exporter = GoogleDocsExporter(auth)

            # Convert job to dict (uses existing Job model!)
            job_data = {
                'id': job.id,
                'created_at': job.created_at,
                'publisher_domain': job.publisher_domain,
                'target_url': job.target_url,
                'anchor_text': job.anchor_text,
                'status': job.status,
                'article_text': job.article_text,
                'qc_report': job.qc_report,
                'actual_cost': job.actual_cost
            }

            # Create Google Doc
            doc_info = docs_exporter.export_job_to_doc(job_data)
            print(f"‚úÖ Google Doc created: {doc_info['document_url']}")

            # Create/Update Google Sheet
            sheet_info = sheets_exporter.create_spreadsheet(
                title="BACOWR Export Example",
                share_with_email=os.getenv("SHARE_WITH_EMAIL")
            )

            # Add job to sheet
            sheets_exporter.export_job(
                spreadsheet_id=sheet_info['spreadsheet_id'],
                job_data=job_data,
                doc_url=doc_info['document_url']
            )
            print(f"‚úÖ Google Sheet updated: {sheet_info['spreadsheet_url']}")

            print("\nüéâ COMPLETE INTEGRATION:")
            print(f"   1. Job processed through BACOWR pipeline")
            print(f"   2. Article generated and stored in database")
            print(f"   3. Exported to Google Docs: {doc_info['document_url'][:50]}...")
            print(f"   4. Added to Google Sheet: {sheet_info['spreadsheet_url'][:50]}...")

    finally:
        db.close()


async def example_2_batch_with_export():
    """
    Example 2: Batch Processing ‚Üí Bulk Export

    Shows how export integrates with batch processing:
    1. Run batch through existing batch_runner
    2. All jobs saved to database
    3. Export entire batch to Google with one call
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 2: Batch Processing with Bulk Export")
    print("=" * 70)

    # Step 1: Run batch (EXISTING BACOWR functionality)
    # Simulating batch completion - in real use, would come from batch_runner.py
    db = SessionLocal()
    try:
        # Get all delivered jobs from recent batch
        jobs = db.query(Job).filter(
            Job.status == "delivered"
        ).limit(10).all()  # Example: last 10 delivered jobs

        if not jobs:
            print("‚ö†Ô∏è  No delivered jobs found. Run some jobs first!")
            return

        print(f"‚úÖ Found {len(jobs)} delivered jobs from batch")

        # Step 2: Export entire batch to Google (NEW functionality)
        auth = GoogleAuthManager()
        sheets_exporter = GoogleSheetsExporter(auth)
        docs_exporter = GoogleDocsExporter(auth)

        # Convert jobs to dict
        job_data_list = []
        for job in jobs:
            job_data_list.append({
                'id': job.id,
                'created_at': job.created_at,
                'publisher_domain': job.publisher_domain,
                'target_url': job.target_url,
                'anchor_text': job.anchor_text,
                'status': job.status,
                'article_text': job.article_text,
                'qc_report': job.qc_report,
                'actual_cost': job.actual_cost
            })

        # Create Google Docs for each
        print(f"üìù Creating {len(jobs)} Google Docs...")
        doc_urls = {}
        for job_data in job_data_list:
            if job_data.get('article_text'):
                doc_info = docs_exporter.export_job_to_doc(job_data)
                doc_urls[job_data['id']] = doc_info['document_url']

        # Export batch to Sheet
        print(f"üìä Creating Google Sheet with all jobs...")
        result = sheets_exporter.export_batch(
            batch_jobs=job_data_list,
            batch_name="Example Batch Export",
            doc_urls=doc_urls,
            share_with_email=os.getenv("SHARE_WITH_EMAIL")
        )

        print(f"\nüéâ BATCH EXPORT COMPLETE:")
        print(f"   Total jobs: {result['total_jobs']}")
        print(f"   Successfully exported: {result['exported']}")
        print(f"   Google Sheet: {result['spreadsheet_url'][:60]}...")
        print(f"   Contains:")
        print(f"      - Batch Summary sheet")
        print(f"      - BACOWR Articles sheet with {len(jobs)} rows")
        print(f"      - Each row links to its Google Doc")

    finally:
        db.close()


async def example_3_api_integration():
    """
    Example 3: Via REST API

    Shows how export is accessible through the unified API.
    All endpoints under /api/v1/export work with existing job data!
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 3: REST API Integration")
    print("=" * 70)

    print("\nExport is fully integrated in the API:")
    print("\n1. Create job (existing endpoint):")
    print("   POST /api/v1/jobs")
    print("   ‚Üí Returns job_id")

    print("\n2. Export to Google (new endpoints):")
    print("   POST /api/v1/export/jobs/{job_id}/google-sheets")
    print("   ‚Üí Uses job data from database")
    print("   ‚Üí Creates Google Sheet + Doc")
    print("   ‚Üí Returns URLs")

    print("\n3. Check status (existing endpoint):")
    print("   GET /api/v1/jobs/{job_id}")
    print("   ‚Üí Shows job status")

    print("\n4. Batch export (new endpoint):")
    print("   POST /api/v1/export/batches/{batch_id}/google-sheets")
    print("   ‚Üí Exports all jobs in batch")

    print("\nüìå Key Point: Export endpoints USE existing job data!")
    print("   They don't create separate data structures.")
    print("   Everything is in the same database, same API.")


async def example_4_automated_workflow():
    """
    Example 4: Automated Export After Job Completion

    Shows how to set up auto-export as part of the job workflow.
    This is the ULTIMATE integration - seamless export!
    """
    print("\n" + "=" * 70)
    print("EXAMPLE 4: Automated Export Workflow")
    print("=" * 70)

    print("\nSetup automated export after every successful job:")
    print("""
# In api/app/routes/jobs.py (existing file):

async def run_job_background(job_id, job_create, db):
    # ... existing BACOWR job execution ...

    # After job completes successfully:
    if job.status == "delivered":
        # Auto-export to Google (optional based on user settings)
        if user.settings.get('auto_export_to_google'):
            from src.export import GoogleSheetsExporter, GoogleDocsExporter

            # Export using existing job data
            exporter = GoogleSheetsExporter()
            exporter.export_job(
                spreadsheet_id=user.settings.get('default_spreadsheet'),
                job_data=job.__dict__
            )
    """)

    print("\n‚úÖ This shows export is PART OF the workflow, not separate!")
    print("   - Uses existing job completion logic")
    print("   - Uses existing user settings")
    print("   - Uses existing database models")
    print("   - Seamless integration!")


async def main():
    """Run all integration examples."""
    print("\n" + "=" * 70)
    print("BACOWR GOOGLE EXPORT - INTEGRATION EXAMPLES")
    print("=" * 70)
    print("\nThese examples show that Google export is FULLY INTEGRATED")
    print("with the existing BACOWR system - not a separate module!")
    print("=" * 70)

    # Check if Google credentials are configured
    creds_path = os.getenv('GOOGLE_CREDENTIALS_PATH', 'credentials/google_credentials.json')
    if not Path(creds_path).exists():
        print("\n‚ö†Ô∏è  Google credentials not found!")
        print(f"   Expected: {creds_path}")
        print("\n   To run these examples:")
        print("   1. Setup Service Account (see src/export/README.md)")
        print("   2. Save credentials to credentials/google_credentials.json")
        print("   3. Set GOOGLE_CREDENTIALS_PATH in .env")
        print("\n   For now, showing API examples only:\n")

        await example_3_api_integration()
        await example_4_automated_workflow()
        return

    # Run all examples
    try:
        await example_1_single_job_with_export()
        await example_2_batch_with_export()
        await example_3_api_integration()
        await example_4_automated_workflow()

        print("\n" + "=" * 70)
        print("‚úÖ ALL EXAMPLES COMPLETE")
        print("=" * 70)
        print("\nKey Takeaways:")
        print("1. Export uses EXISTING job data from database")
        print("2. Export endpoints are in the SAME API (/api/v1/export)")
        print("3. Export can be automated as part of job workflow")
        print("4. Everything is INTEGRATED - one system, not separate pieces!")

    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        print("\nMake sure to:")
        print("1. Have database setup (alembic upgrade head)")
        print("2. Have some completed jobs in database")
        print("3. Have Google credentials configured")


if __name__ == "__main__":
    asyncio.run(main())
